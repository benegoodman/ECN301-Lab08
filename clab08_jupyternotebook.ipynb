{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as norm\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "path = 'defineyourfilepath'\n",
    "os.chdir(path)\n",
    "\n",
    "#\n",
    "# read data from csv-file\n",
    "df = pd.read_csv('hmda.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392616e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['approv'].value_counts())\n",
    "\n",
    "print(df['approv'].mean())\n",
    "\n",
    "print(df['pi_rat'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# estimate probit model\n",
    "probitmod = smf.probit(formula='approv ~ pi_rat', data=df)\n",
    "probitres = probitmod.fit()\n",
    "print(probitres.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "probitape = probitres.get_margeff()\n",
    "print(probitape.summary())\n",
    "papetab = pd.DataFrame(\n",
    "    {'pro_b'   : probitape.margeff,\n",
    "     'pro_se'  : probitape.margeff_se})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f88562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# format the confusion matrix\n",
    "cm_df = pd.DataFrame(probitres.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "print(cm_df)\n",
    "\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "ac = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print()\n",
    "print('The model accuracy is {:.4}'.format(ac))\n",
    "print()\n",
    "print('Baseline accuracy is {:.4}'.format(100*df['approv'].mean()))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e999b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create plot of the CDF for the probit model\n",
    "\n",
    "#\n",
    "# 0 <= pi_rat <= 3\n",
    "x = np.linspace(0,3)\n",
    "y = probitres.params[0] + probitres.params[1]*x\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(4,4))\n",
    "ax.plot(x,norm.cdf(y))\n",
    "ax.set_ylabel('Prob of approval')\n",
    "ax.set_xlabel('PI Ratio')\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "#plt.savefig('fig1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0921dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 0 <= pi_rat <= 2 (better graph)\n",
    "x = np.linspace(0,2)\n",
    "y = probitres.params[0] + probitres.params[1]*x\n",
    "probcdf = norm.cdf(y)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(x,probcdf)\n",
    "ax.set_ylabel('Prob of approval')\n",
    "ax.set_xlabel('PI Ratio')\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "#plt.savefig('fig1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# estimate logit model\n",
    "logitmod = smf.logit(formula='approv ~ pi_rat', data=df)\n",
    "logitres = logitmod.fit()\n",
    "print(logitres.summary())\n",
    "\n",
    "#\n",
    "# marginal effects\n",
    "logitape = logitres.get_margeff()\n",
    "print(logitape.summary())\n",
    "lapetab = pd.DataFrame(\n",
    "    {'log_b'   : logitape.margeff,\n",
    "     'log_se'  : logitape.margeff_se})\n",
    "\n",
    "#\n",
    "# format the confusion matrix\n",
    "cm_df = pd.DataFrame(logitres.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "print(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "ac = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print()\n",
    "print('The model accuracy is {:.4}'.format(ac))\n",
    "print('Baseline  accuracy is {:.4}'.format(100*df['approv'].mean()))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 0 <= pi_rat <= 2 (better graph)\n",
    "x = np.linspace(0,2)\n",
    "y = logitres.params[0] + logitres.params[1]*x\n",
    "logcdf = norm.cdf(y)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(x,logcdf)\n",
    "ax.set_ylabel('Prob of approval')\n",
    "ax.set_xlabel('PI Ratio')\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "#plt.savefig('fig1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3aa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# estimate linear probability model\n",
    "linprmod = smf.ols(formula='approv ~ pi_rat', data=df)\n",
    "linprres = linprmod.fit(cov_type='HC3')\n",
    "print(linprres.summary())\n",
    "\n",
    "#\n",
    "# keep lpm estimate\n",
    "linprtab = pd.DataFrame(\n",
    "    {'lpm_b'   : round(linprres.params, 5),\n",
    "     'lpm_se'  : round(linprres.bse, 5)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a99b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 0 <= pi_rat <= 2 (better graph)\n",
    "x = np.linspace(0,2)\n",
    "lincdf = linprres.params[0] + linprres.params[1]*x\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(x,lincdf)\n",
    "ax.set_ylabel('Prob of approval')\n",
    "ax.set_xlabel('PI Ratio')\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "#plt.savefig('fig1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93893f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# compare\n",
    "fig,ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(x,probcdf)\n",
    "ax.plot(x,logcdf)\n",
    "ax.plot(x,lincdf)\n",
    "ax.set_ylabel('Prob of approval')\n",
    "ax.set_xlabel('PI Ratio')\n",
    "ax.legend(['probit','logit','linear'])\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "#plt.savefig('fig1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99832807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# compare APE results (Wooldridge Table 17.2)\n",
    "\n",
    "#\n",
    "# need some fancy pandas footwork here\n",
    "#  (marginal effects are not indexed with variable names)\n",
    "linprtab = linprtab.drop(['Intercept'])     # drop the intercept for the LPM\n",
    "logprob = lapetab.join(papetab)             # merge logit and probit results\n",
    "logprob = logprob.set_index(linprtab.index) # use the same index\n",
    "\n",
    "apetab = pd.concat([linprtab, logprob], axis=1)\n",
    "print(apetab)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334affb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# estimate probit model\n",
    "#\n",
    "probit2mod = smf.probit(formula='approv ~ pi_rat + white', data=df)\n",
    "probit2res = probit2mod.fit()\n",
    "print(probit2res.summary())\n",
    "\n",
    "#\n",
    "# marginal effects\n",
    "probit2ape = probit2res.get_margeff(dummy=True)\n",
    "print(probit2ape.summary())\n",
    "papetab2 = pd.DataFrame(\n",
    "    {'pro_b'   : probit2ape.margeff,\n",
    "     'pro_se'  : probit2ape.margeff_se})\n",
    "\n",
    "#\n",
    "# format the confusion matrix\n",
    "cm_df = pd.DataFrame(probit2res.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "print(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "ac = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print()\n",
    "print('The model accuracy is {:.4}'.format(ac))\n",
    "print('Baseline  accuracy is {:.4}'.format(100*df['approv'].mean()))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618945e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate probit model\n",
    "#\n",
    "pro_res_wo = smf.probit(formula='approv ~ pi_rat', data=df[df['white']==1]).fit()\n",
    "print(pro_res_wo.summary())\n",
    "\n",
    "pro_res_nw = smf.probit(formula='approv ~ pi_rat', data=df[df['white']==0]).fit()\n",
    "print(pro_res_nw.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# compare\n",
    "wo = pro_res_wo.params[0] + pro_res_wo.params[1]*x\n",
    "cdf_wo = norm.cdf(wo)\n",
    "nw = pro_res_nw.params[0] + pro_res_nw.params[1]*x\n",
    "cdf_nw = norm.cdf(nw)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(x,cdf_wo)\n",
    "ax.plot(x,cdf_nw)\n",
    "ax.set_ylabel('Prob of approval')\n",
    "ax.set_xlabel('PI Ratio')\n",
    "ax.legend(['White','Non-white'])\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "#plt.savefig('fig1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d04428",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrt = -2*(probit2res.llf - (pro_res_wo.llf + pro_res_nw.llf))\n",
    "print('Pooled model: {}'.format(probit2res.llf))\n",
    "print('Whites only : {}'.format(pro_res_wo.llf))\n",
    "print('Non-white   : {}'.format(pro_res_nw.llf))\n",
    "print('LR test     : {}'.format(lrt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# estimate probit model\n",
    "#\n",
    "probit3mod = smf.probit(formula='approv ~ pi_rat + white + hse_inc + ltv_med + ltv_high + ccred + mcred + pubrec + denpmi + selfemp', data=df)\n",
    "probit3res = probit3mod.fit()\n",
    "print(probit3res.summary())\n",
    "\n",
    "#\n",
    "# marginal effects\n",
    "probit3ape = probit3res.get_margeff(dummy=True)\n",
    "print(probit3ape.summary())\n",
    "papetab3 = pd.DataFrame(\n",
    "    {'pro_b'   : probit3ape.margeff,\n",
    "     'pro_se'  : probit3ape.margeff_se})\n",
    "\n",
    "#\n",
    "# format the confusion matrix\n",
    "cm_df = pd.DataFrame(probit3res.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "print(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "ac = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print()\n",
    "print('The model accuracy is {:.4}'.format(ac))\n",
    "print('Baseline  accuracy is {:.4}'.format(100*df['approv'].mean()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3088b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bca6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gof(x_obs, x_pred):\n",
    "    #\n",
    "    # goodness of fit \n",
    "    #\n",
    "    my = np.mean(x_obs)\n",
    "    n_obs = len(x_obs)\n",
    "\n",
    "    #\n",
    "    # classification table\n",
    "    c11 = np.sum(x_obs[x_pred>=0.5])\n",
    "    c10 = np.sum(x_obs[x_pred< 0.5])\n",
    "    c00 = np.sum((1 - x_obs)[x_pred< 0.5])\n",
    "    c01 = np.sum((1 - x_obs)[x_pred>=0.5])\n",
    "\n",
    "    #\n",
    "    # baseline prediction\n",
    "    bpi = 100.0*my\n",
    "    if bpi < 50.0:\n",
    "        bpi = 100.0 - bpi\n",
    "    #\n",
    "    # model accuracy\n",
    "    macc = 100.0*(c00+c11)/n_obs\n",
    "    #\n",
    "    # model precision\n",
    "    mpr0 = 100.0*c00/(c00+c10)\n",
    "    mpr1 = 100.0*c11/(c01+c11)\n",
    "    #\n",
    "    # model recall\n",
    "    mrc0 = 100.0*c00/(c00+c01)\n",
    "    mrc1 = 100.0*c11/(c10+c11)\n",
    "\n",
    "    #\n",
    "    # this is cut and paste from somewhere\n",
    "    # could use a filehandle f here (or edit to use print):    \n",
    "    f = sys.stdout\n",
    "    \n",
    "    #\n",
    "    f.write(\"==========================================\\n\")\n",
    "    f.write(\"                Goodness of fit statistics\\n\")\n",
    "    f.write(\"==========================================\\n\")\n",
    "    f.write(\"Number of observations  : %16d\\n\" % (n_obs))\n",
    "    f.write(\"Baseline predictions    : %16.2f\\n\" % (bpi))\n",
    "    f.write(\"Model accuracy          : %16.2f\\n\" % (macc))\n",
    "    f.write(\"Model precision (pos)   : %16.2f\\n\" % (mpr1))\n",
    "    f.write(\"Model precision (neg)   : %16.2f\\n\" % (mpr0))\n",
    "    f.write(\"Recall (sensitivity)    : %16.2f\\n\" % (mrc1))\n",
    "    f.write(\"Recall (specificity)    : %16.2f\\n\" % (mrc0))\n",
    "    f.write(\"==========================================\\n\\n\")\n",
    "\n",
    "    f.write(\"==========================================\\n\")\n",
    "    f.write(\"   Classification table (Confusion Matrix)\\n\")\n",
    "    f.write(\"==========================================\\n\")\n",
    "    f.write(\"            |         Predicted |         \\n\")\n",
    "    f.write(\"   Observed |        0        1 |    Total\\n\")\n",
    "    f.write(\"------------+-------------------+---------\\n\")\n",
    "    f.write(\"          0 | %8d %8d | %8d\\n\" % (c00, c01, c00+c01))\n",
    "    f.write(\"          1 | %8d %8d | %8d\\n\" % (c10, c11, c10+c11))\n",
    "    f.write(\"------------+-------------------+---------\\n\")\n",
    "    f.write(\"      Total | %8d %8d | %8d\\n\" % (c00+c10, c01+c11, n_obs))\n",
    "    f.write(\"------------+-------------------+---------\\n\\n\")\n",
    "\n",
    "            \n",
    "#\n",
    "do_gof(df['approv'],probit3res.predict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# prepare data for the ROC graph\n",
    "#\n",
    "def get_roc(x_obs, x_prd):\n",
    "    #\n",
    "    # calculate the ROC graph\n",
    "    #\n",
    "\n",
    "    # Note: probably use numpy and pandas\n",
    "    #       rewrite code at some time\n",
    "\n",
    "    #\n",
    "    # gather data\n",
    "    #\n",
    "    dfx = pd.DataFrame(columns=['obs'])\n",
    "    dfx['obs']  = x_obs\n",
    "    dfx['pred'] = x_prd\n",
    "    dfx = dfx.sort_values('pred',ascending=False).copy()\n",
    "    \n",
    "    #\n",
    "    # construct the ROC\n",
    "    rlist = []\n",
    "    pprev = 2\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    for i, v in dfx.iterrows():\n",
    "        if v['pred'] < pprev:\n",
    "            rlist.append([tp,tn])\n",
    "            pprev = v['pred']\n",
    "        if v['obs'] > 0:\n",
    "            tp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "        rlist.append([tp,tn])\n",
    "\n",
    "    #\n",
    "    # convert to pandas dataframe\n",
    "    roc = pd.DataFrame(rlist,columns=['tpr','tnr'])\n",
    "    roc['tpr'] = roc['tpr']/roc['tpr'].iloc[-1]\n",
    "    roc['tnr'] = roc['tnr']/roc['tnr'].iloc[-1]\n",
    "\n",
    "    return roc\n",
    "\n",
    "#\n",
    "#roc = get_roc(df['approv'],probit3res.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ROC data and graph\n",
    "proc = get_roc(df['approv'],probit3res.predict())\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(4,4))\n",
    "ax.plot(proc.tnr,proc.tpr)\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "#lt.savefig('probit_roc.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# function for the area under the ROC graph\n",
    "#\n",
    "\n",
    "def get_aur(x_obs, x_prd):\n",
    "    #\n",
    "    # calculate area under the ROC graph\n",
    "    #\n",
    "\n",
    "    # Note: probably use numpy and pandas\n",
    "    #       rewrite code at some time\n",
    "\n",
    "    #\n",
    "    # gather data\n",
    "    #\n",
    "    dfx = pd.DataFrame(columns=['obs'])\n",
    "    dfx['obs']  = x_obs\n",
    "    dfx['pred'] = x_prd\n",
    "    dfx = dfx.sort_values('pred',ascending=False).copy()\n",
    "    \n",
    "    #\n",
    "    # construct the ROC\n",
    "    pprev = 2\n",
    "    tpc = 0\n",
    "    tnc = 0\n",
    "    tpp = 0\n",
    "    tnp = 0\n",
    "    aur = 0\n",
    "    for i, v in dfx.iterrows():\n",
    "        if v['pred'] < pprev:\n",
    "            aur += abs(tnc-tnp)*(tpc+tpp)*0.5\n",
    "            pprev = v['pred']\n",
    "            tpp = tpc\n",
    "            tnp = tnc\n",
    "        if v['obs'] > 0:\n",
    "            tpc += 1\n",
    "        else:\n",
    "            tnc += 1\n",
    "    aur += abs(tnc-tnp)*(tpc+tpp)*0.5\n",
    "    aur /= (tpc*tnc)\n",
    "\n",
    "    return aur\n",
    "\n",
    "get_aur(df['approv'],probit3res.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ffcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
